{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Scaling-up Framework\n",
    "\n",
    "**Generic framework with BASE_SCALE concept**\n",
    "\n",
    "Framework for scaling up from any base scale unit to different spatial boundaries using area weighting.\n",
    "\n",
    "## Key Concepts\n",
    "- **BASE_SCALE**: The atomic unit (finest resolution) that all other scales aggregate from\n",
    "- **AGGREGATION_SCALES**: Larger spatial boundaries that contain multiple base scale units\n",
    "- **NULL AGGREGATOR**: BASE_SCALE acts as identity aggregator (no spatial aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "from scaling_up_framework_functions import (\n",
    "    pivot_year, aggregate_measure_weighted, standardise_z, normalise,\n",
    "    SpatialScale, validate_geometries, assign_functional_group, validate_csv_data,\n",
    "    plot_base_scale, plot_spatial_hierarchy\n",
    ")\n",
    "from config import BASE_SCALE, GROUP_RULES, INCLUDE_UNMATCHED_TYPES, SPATIAL_FILES, DATA_PATH, OUTPUT_PATH, DEFAULT_CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize data containers\n",
    "base_scale = None  # The atomic unit for all aggregations\n",
    "aggregation_scales = {}  # Higher-level spatial boundaries\n",
    "data_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Scale Data (Atomic Units)\n",
    "\n",
    "**Grouping Behavior:**\n",
    "- `type_field = None`: All features aggregated together\n",
    "- `type_field` defined + `GROUP_RULES`: Uses functional groups\n",
    "- `type_field` defined, no `GROUP_RULES`: Uses original type values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_scale() -> SpatialScale:\n",
    "    \"\"\"Load the base scale - the atomic unit for all aggregations.\"\"\"\n",
    "    base = SpatialScale(\n",
    "        name=BASE_SCALE[\"name\"],\n",
    "        source=BASE_SCALE[\"file\"],\n",
    "        unique_id_field=BASE_SCALE[\"unique_id\"],\n",
    "        measure_field=BASE_SCALE[\"measure_field\"],\n",
    "        type_field=BASE_SCALE[\"type_field\"],\n",
    "        aggregation_method=BASE_SCALE.get(\"aggregation_method\", \"geometry\"),\n",
    "        is_base_scale=True\n",
    "    )\n",
    "    \n",
    "    # Validate geometries\n",
    "    base.data = validate_geometries(base.data)\n",
    "    \n",
    "    # Handle grouping based on type_field availability\n",
    "    # This determines how features are grouped for separate analysis\n",
    "    if BASE_SCALE.get(\"type_field\") is None:\n",
    "        print(\"WARNING: type_field not defined - all features will be aggregated together\")\n",
    "        print(\"Result: Single output per spatial scale (no sub-grouping)\")\n",
    "        base.data[\"grp\"] = \"all_features\"\n",
    "    else:\n",
    "        # Assign functional groups or use original type field as fallback\n",
    "        base.data[\"grp\"] = base.data[BASE_SCALE[\"type_field\"]].apply(\n",
    "            lambda x: assign_functional_group(x, BASE_SCALE[\"name\"], GROUP_RULES, INCLUDE_UNMATCHED_TYPES)\n",
    "        )\n",
    "        # If no functional groups assigned, use original type field\n",
    "        if base.data[\"grp\"].isna().all():\n",
    "            print(f\"No functional groups defined for {BASE_SCALE['name']} - using original type field\")\n",
    "            base.data[\"grp\"] = base.data[BASE_SCALE[\"type_field\"]]\n",
    "        else:\n",
    "            print(f\"Using functional groups for {BASE_SCALE['name']}\")\n",
    "    \n",
    "    return base\n",
    "\n",
    "# Load base scale data\n",
    "base_scale = load_base_scale()\n",
    "print(f\"BASE_SCALE: {base_scale}\")\n",
    "\n",
    "# Plot base scale\n",
    "plot_base_scale(base_scale, BASE_SCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Aggregation Scale Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregation_scale(name: str, source_key: str, unique_field: str) -> pd.DataFrame:\n",
    "    \"\"\"Create aggregation scale by spatially joining with base scale.\"\"\"\n",
    "    agg_scale = SpatialScale(name, SPATIAL_FILES[source_key], unique_field)\n",
    "    \n",
    "    # Spatial join with base scale\n",
    "    joined = gpd.sjoin(\n",
    "        base_scale.data, agg_scale.data,\n",
    "        how=\"left\", predicate=\"intersects\"\n",
    "    ).dropna().set_index(BASE_SCALE[\"unique_id\"])\n",
    "    \n",
    "    return joined\n",
    "\n",
    "# Define aggregation scales\n",
    "AGGREGATION_CONFIGS = {\n",
    "    \"Valley\": {\"source_key\": \"Valley\", \"unique_field\": \"BWS_Region\"},\n",
    "    \"DIWA\": {\"source_key\": \"DIWA\", \"unique_field\": \"WNAME\"},\n",
    "    \"Ramsar\": {\"source_key\": \"Ramsar\", \"unique_field\": [\"RAMSAR_NAM\", \"WETLAND_NA\"]},\n",
    "    \"NorthSouthBasin\": {\"source_key\": \"NorthSouthBasin\", \"unique_field\": \"Region\"}\n",
    "}\n",
    "\n",
    "# Load all aggregation scales\n",
    "agg_scale_objects = {}\n",
    "for scale_name, config in AGGREGATION_CONFIGS.items():\n",
    "    aggregation_scales[scale_name] = create_aggregation_scale(\n",
    "        scale_name, config[\"source_key\"], config[\"unique_field\"]\n",
    "    )\n",
    "    agg_scale_objects[scale_name] = SpatialScale(scale_name, SPATIAL_FILES[config[\"source_key\"]], config[\"unique_field\"])\n",
    "    print(f\"AGGREGATION_SCALE {scale_name}: {len(aggregation_scales[scale_name])} intersections\")\n",
    "\n",
    "# Plot all scales together\n",
    "plot_spatial_hierarchy(base_scale, BASE_SCALE, agg_scale_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Metric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metric_data(metric_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Load metric data from CSV file or spatial data attribute.\"\"\"\n",
    "    if metric_name in data_cache:\n",
    "        return data_cache[metric_name]\n",
    "    \n",
    "    print(f\"Loading {metric_name} data...\")\n",
    "    \n",
    "    # First try to find CSV file\n",
    "    csv_files = list(DATA_PATH.glob(f\"*{metric_name}*.csv\"))\n",
    "    \n",
    "    if csv_files:\n",
    "        # Load from CSV file\n",
    "        csv_file = csv_files[0]\n",
    "        print(f\"Found CSV: {csv_file.name}\")\n",
    "        data = pd.read_csv(csv_file)\n",
    "        data = validate_csv_data(data, metric_name, BASE_SCALE[\"unique_id\"])\n",
    "    else:\n",
    "        # Try to load from spatial data attribute\n",
    "        print(f\"No CSV found, checking if '{metric_name}' exists in spatial data...\")\n",
    "        if metric_name not in base_scale.data.columns:\n",
    "            raise FileNotFoundError(f\"Metric '{metric_name}' not found in CSV files or spatial data columns.\\n\"\n",
    "                                  f\"Available spatial columns: {list(base_scale.data.columns)}\")\n",
    "        \n",
    "        print(f\"Using '{metric_name}' from spatial data\")\n",
    "        # Create DataFrame from spatial data\n",
    "        data = base_scale.data[[BASE_SCALE[\"unique_id\"], metric_name]].copy()\n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "        # Validate metric column is numeric\n",
    "        if not pd.api.types.is_numeric_dtype(data[metric_name]):\n",
    "            data[metric_name] = pd.to_numeric(data[metric_name], errors='coerce')\n",
    "            nan_count = data[metric_name].isna().sum()\n",
    "            if nan_count > 0:\n",
    "                print(f\"Warning: {nan_count} non-numeric values converted to NaN\")\n",
    "                data = data.dropna(subset=[metric_name])\n",
    "    \n",
    "    # Normalize values\n",
    "    data[metric_name] = normalise(data[metric_name])\n",
    "    \n",
    "    data_cache[metric_name] = data\n",
    "    return data\n",
    "\n",
    "# Load NDVI data\n",
    "ndvi_data = load_metric_data(\"NDVI\")\n",
    "print(f\"Loaded NDVI data: {len(ndvi_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_scales(data: pd.DataFrame, metric_name: str) -> None:\n",
    "    \"\"\"Process metric across all spatial scales.\"\"\"\n",
    "    \n",
    "    # Create pivot table with baseline statistics\n",
    "    pivot_data = pivot_year(data, metric_name, BASE_SCALE[\"unique_id\"])\n",
    "    \n",
    "    # Process BASE_SCALE (null aggregator)\n",
    "    print(f\"Processing {metric_name} for BASE_SCALE ({BASE_SCALE['name']})...\")\n",
    "    base_result = aggregate_measure_weighted(\n",
    "        pivot_data, None, [], base_scale.measure_field, [\"grp\"], is_base_scale=True\n",
    "    )\n",
    "    output_file = OUTPUT_PATH / f\"{metric_name}_{BASE_SCALE['name']}.csv\"\n",
    "    base_result.round(4).to_csv(output_file)\n",
    "    print(f\"Saved: {output_file}\")\n",
    "    \n",
    "    # Process each AGGREGATION_SCALE\n",
    "    for scale_name, aggregator in aggregation_scales.items():\n",
    "        print(f\"Processing {metric_name} for AGGREGATION_SCALE ({scale_name})...\")\n",
    "        \n",
    "        agg_result = aggregate_measure_weighted(\n",
    "            pivot_data, aggregator, [scale_name], base_scale.measure_field, [\"grp\"]\n",
    "        )\n",
    "        \n",
    "        output_file = OUTPUT_PATH / f\"{metric_name}_{scale_name}.csv\"\n",
    "        agg_result.round(4).to_csv(output_file)\n",
    "        print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Process NDVI across all scales\n",
    "process_all_scales(ndvi_data, \"NDVI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scaling hierarchy\n",
    "print(\"\\n=== SPATIAL SCALING HIERARCHY ===\")\n",
    "print(f\"BASE_SCALE (atomic units): {base_scale}\")\n",
    "print(\"\\nAGGREGATION_SCALES:\")\n",
    "for name, data in aggregation_scales.items():\n",
    "    print(f\"  {name}: {len(data)} base units mapped\")\n",
    "\n",
    "# Display year range if year column exists\n",
    "if 'year' in ndvi_data.columns:\n",
    "    print(f\"\\nMetric data years: {ndvi_data['year'].min()}-{ndvi_data['year'].max()}\")\n",
    "else:\n",
    "    print(f\"\\nMetric data: Single time point (no year column)\")\n",
    "\n",
    "print(f\"Output files: {len(list(OUTPUT_PATH.glob('*.csv')))}\")\n",
    "\n",
    "# List output files by scale\n",
    "print(\"\\nOutput files by scale:\")\n",
    "for file in sorted(OUTPUT_PATH.glob(\"*.csv\")):\n",
    "    scale_type = \"BASE_SCALE\" if BASE_SCALE[\"name\"] in file.name else \"AGGREGATION_SCALE\"\n",
    "    print(f\"  {scale_type}: {file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
